import math
import torch
import copy as cp
import numpy as np
from typing import Tuple, List, Dict, OrderedDict, Optional, Union
from sklearn.decomposition import PCA
from scipy.fftpack import dct


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Type alias for state dicts
StateDict = Dict[str, torch.Tensor]

def flatten_weights(state_dict: StateDict) -> Tuple[np.ndarray, List[Tuple[str, torch.Size, torch.dtype]]]:
    """
    Flattens a PyTorch state dictionary into a single 1D NumPy array.

    Args:
        state_dict (Dict[str, torch.Tensor]): The PyTorch model state dictionary
                                             or a dictionary of update tensors.

    Returns:
        Tuple[np.ndarray, List[Tuple[str, torch.Size, torch.dtype]]]:
            - A 1D NumPy array containing all flattened parameters.
            - A list of metadata tuples (key, shape, dtype) needed for unflattening.
    """
    flat_params = []
    metadata = []
    # Ensure consistent order if input might be a regular dict
    keys = state_dict.keys()
    if isinstance(state_dict, OrderedDict):
        keys = state_dict.keys()
    # else: # For regular dict, sort keys for consistency (optional but recommended)
    #     keys = sorted(state_dict.keys())

    for key in keys:
        param = state_dict[key]
        metadata.append((key, param.shape, param.dtype))
        # Ensure tensor is on CPU before converting to numpy
        flat_params.append(param.detach().cpu().numpy().flatten())

    if not flat_params: # Handle empty state_dict
        return np.array([]), []

    return np.concatenate(flat_params), metadata

def unflatten_weights(flat_vector: np.ndarray,
                      metadata: List[Tuple[str, torch.Size, torch.dtype]]) -> StateDict:
    """
    Unflattens a 1D NumPy array back into a PyTorch state dictionary using metadata.

    Args:
        flat_vector (np.ndarray): The 1D NumPy array of flattened parameters.
        metadata (List[Tuple[str, torch.Size, torch.dtype]]): Metadata list generated by
                                                             flatten_weights, containing
                                                             (key, shape, dtype) for each parameter.

    Returns:
        Dict[str, torch.Tensor]: The reconstructed PyTorch state dictionary.
    """
    state_dict = OrderedDict() # Use OrderedDict to preserve original order
    current_index = 0
    for key, shape, dtype in metadata:
        num_elements = torch.prod(torch.tensor(shape)).item()
        # Slice the flat vector
        param_flat = flat_vector[current_index : current_index + num_elements]
        # Reshape and convert back to PyTorch tensor with original dtype
        param = torch.from_numpy(param_flat.reshape(shape)).to(dtype)
        state_dict[key] = param
        current_index += num_elements

    # Check if we used the whole vector
    if current_index != len(flat_vector):
        print(f"Warning: Vector length ({len(flat_vector)}) did not match expected total elements ({current_index}).")

    return state_dict

def reduce_dimension_pca(data: np.ndarray,
                         n_components: Union[int, float]) -> Optional[np.ndarray]:
    """
    Reduces the dimensionality of the data using PCA.

    Args:
        data (np.ndarray): The input data array, shape (n_samples, n_features).
                           Typically, n_samples is the number of client updates,
                           and n_features is the number of flattened parameters.
        n_components (Union[int, float]):
            - If int: The target number of principal components to keep.
            - If float (between 0.0 and 1.0): The minimum fraction of variance
              that the selected components should explain.

    Returns:
        Optional[np.ndarray]: The dimension-reduced data, shape (n_samples, n_components_kept),
                              or None if reduction fails or input is invalid.
    """
    if data.ndim != 2:
        print("Error: PCA requires 2D input data (n_samples, n_features).")
        return None

    n_samples, n_features = data.shape

    if n_samples == 0 or n_features == 0:
        print("Warning: PCA input data is empty.")
        return np.empty((n_samples, 0)) # Return appropriately shaped empty array

    pca_param = n_components # Store the user's request

    # --- Validate n_components ---
    if isinstance(n_components, int):
        if n_components <= 0:
            print("Warning: Number of PCA components (int) must be positive.")
            return None
        # Limit components by available dimensions
        max_components = min(n_samples, n_features)
        if n_components > max_components:
            print(f"Warning: Requested n_components ({n_components}) exceeds max possible ({max_components}). Clamping to {max_components}.")
            pca_param = max_components
        elif n_components == n_features:
             # Scikit-learn PCA with n_components == n_features can be useful (e.g., for whitening)
             # but doesn't reduce dimensionality. Let's allow it but warn.
             print(f"Info: n_components ({n_components}) == n_features ({n_features}). PCA will not reduce dimensionality in this case.")
             # If strictly for reduction, could return data here: return data

    elif isinstance(n_components, float):
        if not (0.0 < n_components <= 1.0):
            print("Warning: PCA variance ratio (float) must be between 0.0 (exclusive) and 1.0 (inclusive).")
            return None
        # Scikit-learn handles the float case directly.
        # It selects the number of components such that the amount of variance
        # that needs to be explained is greater than the percentage specified by n_components.
        # Note: If n_components=1.0, it keeps all components where explained variance > 0.
        pca_param = n_components
    else:
        print("Error: n_components must be an integer or a float between 0.0 and 1.0.")
        return None

    # --- Perform PCA ---
    try:
        # Note: PCA centers the data by default (mean subtraction)
        pca = PCA(n_components=pca_param)
        reduced_data = pca.fit_transform(data)

        n_components_kept = pca.n_components_
        total_variance_explained = pca.explained_variance_ratio_.sum() if hasattr(pca, 'explained_variance_ratio_') else float('nan')

        print(f"PCA reduced data from ({n_samples}, {n_features}) to ({reduced_data.shape}) using n_components={pca_param}.")
        print(f"Actual components kept: {n_components_kept}. Total explained variance: {total_variance_explained:.4f}")

        # Check if reduction actually happened (relevant if n_components was int >= n_features)
        if reduced_data.shape[1] == n_features and isinstance(n_components, int) and n_components >= n_features :
             print("Info: PCA did not reduce dimensionality as requested components >= features.")
             # Optionally return original data if no reduction was the goal: return data

        return reduced_data
    except ValueError as ve:
         # Catch specific errors, e.g., n_components > min(n_samples, n_features) if not handled above
         print(f"Error during PCA setup or execution: {ve}")
         return None
    except Exception as e:
        print(f"An unexpected error occurred during PCA: {e}")
        return None
    
    
def reduce_dimension_dct(data: np.ndarray, n_components: int) -> Optional[np.ndarray]:
    """
    Reduces the dimensionality of each sample (row) in the data using DCT Type II.
    Keeps the first `n_components` coefficients for each sample.

    Args:
        data (np.ndarray): The input data array, shape (n_samples, n_features).
        n_components (int): The number of DCT coefficients to keep for each sample.

    Returns:
        Optional[np.ndarray]: The dimension-reduced data, shape (n_samples, n_components),
                              or None if reduction is not possible.
    """
    if data.ndim != 2:
        print("Error: DCT reduction requires 2D input data (n_samples, n_features).")
        return None
        
    n_samples, n_features = data.shape

    # Ensure n_components is valid
    effective_n_components = min(n_components, n_features)
    if effective_n_components <= 0:
        print("Warning: Number of DCT components must be positive.")
        return None
    if effective_n_components > n_features: # Cannot keep more coeffs than exist
         print(f"Warning: n_components ({n_components}) > n_features ({n_features}). Clamping to {n_features}.")
         effective_n_components = n_features

    try:
        # Apply DCT row-wise (axis=1) and keep the first k components
        dct_transformed = dct(data, type=2, axis=1, norm='ortho')
        reduced_data = dct_transformed[:, :effective_n_components]
        print(f"DCT reduced data from ({n_samples}, {n_features}) to ({reduced_data.shape})")
        return reduced_data
    except Exception as e:
        print(f"Error during DCT: {e}")
        return None

def compute_weighted_average(updates: list[dict], weights=None) -> dict:
    """
    computes the average update from a list of updates.
    Args:
        updates (list[dict]): the list of updates to average.
        weights (list[float]): the weight of each update during averaging.
    Returns:
        dict: the weighted average of updates.
    """
    if weights is None:
        weights = [1] * len(updates)
    sum_weights = np.sum(weights)
    avg = cp.deepcopy(updates[0])
    for key in avg.keys():
        avg[key] = sum(param[key] * (weight / sum_weights) for param, weight in zip(updates, weights))
    return avg

def compute_diff(w1, w2):
    """
    computes the difference between two updates (grad). the difference is computed between weights and biases
    other keys are ignored.
    Args:
        w1 (dict): the first model update
        w2 (dict): the second model update
    Returns:
        dict: w1 - w2
    """
    res = cp.deepcopy(w1)
    for key in res.keys():
        if "weight" in key or "bias" in key:
            res[key] = w1[key] - w2[key]
        else:
            res[key].zero_()    
    return res

def get_direction(updates, ref_update, alpha=0.999, print_stats=False):
    """
    returns the change of direction between the average of a list of updates (potential next global update)
    and a reference model (previous global model)
    """
    avg = compute_weighted_average(updates)
    diff = compute_diff(avg, ref_update) # avg - ref_update
    directions = cp.deepcopy(diff)

    thresholds = {key: np.percentile(np.abs(diff[key].cpu().detach().numpy()), (1 - alpha) * 100) for key in diff.keys()}
    num_zeros, num_negatives, num_positives = 0, 0, 0
    for ky in diff.keys():
        grad_array = diff[ky].cpu().detach().numpy()
        direction_array = np.sign(np.where(np.abs(grad_array) > thresholds[ky], grad_array, 0))
        directions[ky] = torch.from_numpy(direction_array).to(device)

        num_zeros += np.sum(direction_array == 0)
        num_negatives += np.sum(direction_array == -1)
        num_positives += np.sum(direction_array == 1)

    if print_stats:
        total = num_zeros + num_negatives + num_positives
        print(f"* Stat of Directions: {num_negatives / total * 100:.2f}% -1, {num_zeros / total * 100:.2f}% 0, {num_positives / total * 100:.2f}% 1")

    return directions   

def compute_distance(w0, w1) -> float:
    """ 
    computes the ditance between two updates. 
    Args:
        w0 (dict): first update.
        w1 (dict): second update.
    Returns
        float: ||w0 - w1||Â²
    """
    dist = 0.0
    for key in w1.keys():
        if "weight" in key or "bias" in key:
            dist += np.linalg.norm(w0[key].cpu().detach().numpy() - w1[key].cpu().detach().numpy()) ** 2
    return dist

def compute_l2_norm(update: dict)-> float:
	"""
	Compute the L2 norm of an update.
	Args:
        update (dict): A dictionary representing an update (e.g., model state_dict).
    Returns:
        float: The L2 norm of the update.
    """
	norm_sq = 0.0
	for key in update.keys():
		norm_sq += torch.sum(update[key]**2).item()
	l2_norm = math.sqrt(norm_sq)
	return l2_norm

class NoiseDataset(torch.utils.data.Dataset):
    """Dataset that generates random noise inputs."""

    def __init__(self, size: Tuple[int, int, int], num_samples: int):
        self.size = size
        self.num_samples = num_samples

    def __len__(self) -> int:
        return self.num_samples

    def __getitem__(self, idx: int) -> torch.Tensor:
        # Return just the noise tensor without a label
        noise = torch.rand(self.size)
        return noise
